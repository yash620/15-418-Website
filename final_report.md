#  Midpoint Report: B+Trees for In-Memory Databases using Hardware Transactional Memory

## Summary
We implemented an optimized B+Tree using restricted transactional memory (RTM) and compared the performance with a more traditional optimistic lock coupling approach. Our deliverables include graphs and performance analysis indicating the benefits and drawbacks of RTM vs optimistic locking along with our HTM B+Tree implementation. We ran our experiments on a 2-socket machine that supports Intel® TSX® with two Intel® Xeon® Silver 4114 processors with 10 cores and 20 hyperthreads each. 

## Background
B+Tree are in memory data structures that are used to index data in databases and file systems. For this project we focused on their uses cases in in-memory databases. B+Tree are a self balancing tree where each node can have at most n children. They ensure balancing by having an invariant that all leaves are the same distance from the root node. B+Trees (Figure 2) are different from B-Trees (Figure 1)  in that in B+Tree inner nodes only store keys used to navigate down the tree. The value associated with the keys is stored within the leaf nodes, and all keys are represented within the leaf nodes. To allow for efficient lookup the keys within each node are sorted in increasing order. This way when traversing the tree at each node the subtree containing a key can be retrieved by doing a binary search on the ranges of the children.

![B-Tree](images/BTree.png)
*Figure 1: B-Tree*
![B+Tree](images/B+Tree.png)
*Figure 2: B+Tree*

Within in-memory databases B+Trees are used to build indexes on tables. Indexes are intended to allow for efficient data retrieval based on specific attributes of the tuples contained in the table. For this use case the keys are attributes within the tuple and the value is an address pointing to the location of the tuple. Database indexes provide operations such as insert, lookup, delete, and scan but for this project we focused on optimizing insert and lookup. 

Insert takes a key and a value as parameters and traverses the tree to insert key at the bottom of the tree. When inserting though the algorithm has to ensure that no node exceeds its maximum number of children, which it does so by splitting full nodes. A node split involves creating a new node, moving half the keys in the splitting node into the new node, and then adding the new node as a child of the parent. If the root node is split then a new root node is created as well and the old root node and it’s split node become children of the new root node. There are two common approaches of when to split, eager and lazy splitting. In an eager splitting approach the program traversing down the tree will split a node whenever it detects that the node is at capacity. In a lazy approach the key being inserted will be inserted at a leaf and then the leaf is split only if it exceeds capacity and then ancestor nodes are split accordingly if they are full as well. 

Lookups are more straightforward in that they don’t make any modifications. Lookup takes a key as a parameter and returns the value associated with the key. The function traverses the tree, doing a binary search at each node to find the corresponding subtree to traverse down. Once it reaches a leaf node it returns the value associated with the key found there. 

Database workloads often involve querying the index multiple times and launching a query doesn’t have data dependencies on launching other queries. Therefore it is possible to parallelize the workload by running multiple queries on an index concurrently. With inter-query parallelism lookups are dependent on the current node the program is operating on remains unchanged. Inserts, with eager splitting, only require that the current node and it’s parent must not be modified. The parent is a dependency since when splitting a new child is inserted into the parent. With lazy splitting all of the ancestors of the leaf node could be modified, as a split can recursively cause splits all the way up to the root node. Intra query parallelization is difficult since identifying the next node on which to continue the traversal requires finishing a binary search on the current node.  

Within a query there isn’t much cache locality as each step involves a node which is a new data location. Across queries though it is possible to improve cache locality by reordering queries in a way that queries that taking similar paths down the tree would be grouped together. But this would require complex static analysis identifying possibilities for index operation reordering without breaking cross query data dependencies and it would also require knowledge of the operations being run before running them. For this project though we chose to focus more on online operations, where not all the operations are known beforehand, and on parallelizing the data structure rather than on analysis and reordering of the inputs. 

## Approach
### Initial Implementation
We started off with an optimistic lock coupling (OLC) based [approach](https://github.com/wangziqi2016/index-microbench/blob/master/BTreeOLC/BTreeOLC.h) by Dr. Viktor Leis from Technische Universität München. Lock coupling involves holding crabbing down the tree while holding two locks (the locks for the parent and the current node) at a time. Whenever moving on from a node to a child the lock on the parent is released and the lock for the child is grabbed. Optimistic means that readers don’t block or take locks, instead when they would normally release a lock they would verify if the version of the node matches the version they initially read. If the versions changed then they would retry the operation again as the data has changed. Writers do take locks on the node and when they release the lock they update a version counter to indicate that the data was modified. On lookups only read locks would be taken. On writes, write locks only need to be taken when splitting a node or when inserting into the leaf node. For the rest of the traversal only read locks are needed.

We started off with this implementation as OLC is one of the most common parallel implementations for B-Trees and it takes a similar approach to that of restricted transactional memory (RTM). Intel TSX, the RTM instruction set we used, provides three instructions to enable transactionality: 
1. _xbegin: begins a transactional region
2. _xend: ends a transactional region
3. _xabort: aborts the current running transaction.

RTM operates by enabling sections of code to operate with transactional semantics. This means that with respect to other transactional sections each section operates as if it was isolated and atomic. This is achieved by adding all variables read to a transactions read set and all variables written to a write set. Whenever a transaction is committed a coherence notification is sent out invalidating the variables in the write set. If a current running transaction sees that the variables in it’s read and write sets are invalidated then it aborts. This is similar to OLC in that readers don’t block but writers invalidate other transactions. But unlike OLC approaches RTM isn’t guaranteed to make progress. To ensure progress it needs a fallback path that it can execute that doesn’t use RTM. 

For our initial implementation we created a single threaded version of the B+Tree and encased each insert and lookup operation in an RTM transaction. Then we used the latch coupling implementation (not optimistic) as a fallback path which would be taken if the RTM transaction restarted too many times. More analysis about picking the maximum number of restarts is in the results section. As we began testing we noticed that the transactions were aborting ~80% of the time and resorting to the fallback path. We experimented with varying datasets and found that ordering inserts by increasing order of keys would only cause transactions to abort on node split as we expected. We realized this was because when key was being inserted into a node if the key was in the middle of the range of keys then the existing keys in the node would be shifted over to maintain sorted order. Shifting these keys over exceeded the memory limits of an RTM transaction causing it to abort. To address this we limited each node’s maximum number of entries to 31, as shifting that many keys was the memory limit for HTM transactions. 

### Addressing Concurrency
At this stage though we encountered corrupted data as there were race conditions while executing the operations. The RTM transaction was thread safe with respect to other RTM transactions and the fallback latch coupling implementation was thread safe with respect to itself. But they weren’t thread safe with respect to each other. For instance, even if a latch coupled traversal has taken a lock and is reading a node the RTM implementation could modify the data as it has no notion of a lock being taken. To fix this we had to add the lock of each node touched in an RTM traversal to the read set of the transaction. The RTM transaction would check if the lock is taken before going to a node, if it is taken then it would abort and retry. Also if the fallback path of another thread takes the lock it would execute a write to the lock variable which in turn would cause all transactions who have the lock in their read set to abort. This ensures that the RTM transaction and the latch coupled fallback path are thread safe with respect to each other. 

These changes allowed our implementation to be correct but it wasn’t performant. The non-optimistic latch coupling path would take too many locks while traversing the tree and cause many transactions to abort. Whenever a new traversal was started it would take and release a lock on the root node, which involves writing to the lock. All running RTM transactions would have the lock for the root node in their read set and thus would be aborted by this write. 
To get around this bottleneck we changed our fallback path to be an OLC implementation. In these cases the fallback path would only write to a node’s lock when modifying the node, therefore only aborting running RTM transactions when necessary. But this change brought back data corruption and race conditions.

Having an OLC based fallback complicated the interaction between the RTM path and the fallback path. Now there were 4 possible paths of interaction between RTM and OLC at a specific node: 
1. RTM Read and OLC Write 
2. RTM Read and OLC Read
3. RTM Write and OLC Write
4. RTM Write and OLC Read. 

RTM Read and OLC Write is addressed much like the synchronization in the non optimistic fall back implementation with the RTM checking if the node’s lock is free aborting if it isn’t and adding it to the read set if it is. If the OLC write starts first then the RTM read will see it and abort as the lock was taken. If the RTM read start first the OLC write will invalidate the lock and since it was in the RTM read set the transaction would get aborted. 

In the case of RTM Read and OLC Read both can safely run concurrently so no extra synchronization is required. 

The RTM Read and OLC Write case is solved in the same way as RTM Read and OLC Write with the RTM checking if the node’s lock is free. If RTM is committed before OLC Write then OLC write will see RTM write and will operate accordingly. If OLC write occurs before RTM is committed but after RTM has read the node then OLC will invalidate the node’s lock which is in the RTM read set and the RTM transaction gets aborted. 

The RTM Write and OLC Read case wasn’t solved by using the same synchronization approach as the non-optimistic latch coupling implementation and was the cause of our data corruption and race condition. There is a possible race in that the OLC traversal has read a node but before it could release it’s read lock and validate that the node version hasn’t changed the RTM transaction could write to the node. In our first implementation the RTM transaction wasn’t modifying a node’s locks only reading them, so the OLC traversal would see that the node’s version is the same and validate the read even though it wasn’t a valid read. To fix this we needed the RTM transaction as well to update the node’s version number when it was writing to that node. 

### Optimizations
At this point we got correctness and performance but there are two more optimizations we attempted. The first was that we didn’t want to limit a node’s maximum number of keys to 31 as this would cause more splits and thus the RTM transaction to fail more often. For inner nodes this was unavoidable but for leaf nodes, where most of the inserts would be happening, we theorized that a node’s keys don’t need to be sorted until that node is read. On inserts we would append keys to the existing list of keys at the node without shifting anything, and thus not abort an RTM transaction. Whenever a node was split or on a lookup we would sort the keys in the leaf node if it wasn’t sorted already. Our intuition was that an RTM transaction would be aborted anyway on a split, and by increasing the size of the node splits would be rarer. On a lookup only the first lookup after an insert would need to sort the keys in the node. With this we could increase the maximum entries in a node beyond 31 and in the results section we provide more analysis on how changing node size affected performance.

The second optimization we attempted was limiting the amount of data an RTM transaction would touch. Instead of enclosing the entire insert or lookup operation in an RTM transaction we weaved transactions by enclosing only the operations done at each node while traversing the tree into a transaction. So while traversing the tree the program would begin a transaction, do the operation needed at the current node, either a binary search to find the next node to go to, an insert or a split and then end the transaction once the operation at the node was completed. And then begin a new transaction at the new node. During this process though we had to check that nodes that were being operated on didn’t change between the two transactions. Each node already had a version number to enable OLC so we just had to verify that the version was the same between the two transactions. If the version was different then we would retry the insert or lookup. We theorized that by limiting the size of a transaction there would be fewer aborts of transactions and the fallback path would be used less. The analysis for this is further expanded on in the results section. 

## Results
In this section we will present the results evaluating the RTM B+Tree performance in differing conditions. All experiments were run on a 2-socket machine two Intel® Xeon® Silver 4114 processors with 10 cores and 20 hyperthreads each and 125 GB of memory. All values are the minimum across 5 runs. Speedup values are based on a baseline single threaded implementation.

### Workload
To evaluate performance with changing workloads we created a workload generator that would vary the ratio of inserts to lookups. The generator would take in number of operations (N) and the percentage of operations to be inserts (P) as parameters. Each operation could be a lookup or insert and in order to simulate a more realistic workload when generating each operation the operator would pick either an insert or a lookup from a weighted binomial distribution where an insert is selected with P probability. For multithreaded workloads each thread was given a chunk of the total workload and each chunk would only have lookups of values also inserted within that chunk. For lookup only workloads the inserts would occur beforehand and only inserts would be timed. 

### System Parameter Evaluations
There were two system wide parameters that we had to set. The first is how many times should an RTM transaction be allowed to restart before taking the fallback path. And the second is what is the maximum number of entries a leaf node should contain. 

